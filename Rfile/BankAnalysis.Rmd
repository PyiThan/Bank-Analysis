---
title: "The Data Driven Approach to predict the success of Bank Telemarketing"
output: html_document
date: "2023-05-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the library

```{r,echo=FALSE,results="hide",warning=FALSE,message=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(glmnet)
library(pls)
library (e1071)
library(corrplot)
library(tree)
library(ipred)
library(rpart)
library(gam)
library(randomForest)
library(gbm)
library(caret)
library(class)
library(FNN)
library(MASS)
library(DMwR2)
library(pROC)
library(Epi)
library(ROSE)
library(png)
library(tibble)
library(pROC)
```

### Load the dataset

```{r}
Bank=read.csv("~/Desktop/SFSU/math449Project/bank.csv",header=TRUE,sep=";")
```

### To see the first five row of the data

```{r}
head(Bank)
```

### To see the data type of Bank.csv

```{r}
str(Bank)
```

### Correlatin between numeric predictors variable

```{r}
BankNumeric=select_if(Bank,is.integer)
corMatrix=cor(BankNumeric)
corrplot(corMatrix,type="upper",method="color",tl.col="black",t1.srt=45)
```

The correlation coefficients between all pairs of predictor variables in the model are less than 0.5. Thus, we don't need to worry about multicollinearity in this problem.

### Value counts for response variable

```{r}
countsY=table(Bank$y)
barplot(countsY,main="Frequency for response variable Y",xlab="Customer Subsribe the term Deposit",ylab="Frequency")
```

### Compute the percentage of yes and no

```{r}
percentageYes=countsY[2] / nrow(Bank) * 100
percentageNo=countsY[1] / nrow(Bank) * 100

cat(paste0("Percentage of subscription of term deposit: ",format(percentageYes,nsmall=2), " %"))
cat("\n")
cat(paste0("Percentage of no subscription of term deposit: ",format(percentageNo,nsmall=2), " %"))
```

### I will try to make it balance by using the oversampling method

```{r}
Bank=ovun.sample(y~.,data=Bank,method="both",N=nrow(Bank),seed=123)$data
table(Bank$y)
```

Now, the response variable is balanced. . \### convert the response variable into binary 0 means no and 1 means yes

```{r}
Bank$y=ifelse(Bank$y == "yes",1,0)
table(Bank$y)
```

Now, the response variable is balanced.

### Convert the non numeric vairables into factor

```{r}
Bank$job=as.factor(Bank$job)
Bank$marital=as.factor(Bank$marital)
Bank$education=as.factor(Bank$education)
Bank$default=as.factor(Bank$default)
Bank$housing=as.factor(Bank$housing)
Bank$loan=as.factor(Bank$loan)
Bank$contact=as.factor(Bank$contact)
Bank$month=as.factor(Bank$month)
Bank$poutcome=as.factor(Bank$poutcome)
Bank$y=as.factor(Bank$y)
```

### Creating training and testing data from the original data

```{r}
trainFull = sample(dim(Bank)[1], dim(Bank)[1]*0.8)
testFull=-trainFull
Bank.fullTest=Bank[testFull,]
Bank.fullTrain=Bank[trainFull,]
```

## (1) Fit a logistic regression model with all predictors

```{r}
library(caret)
fitControl <- trainControl(method = "cv", number = 10)

fullBankFit <- train(y ~ ., data = Bank.fullTrain , method = "glm", trControl = fitControl,family=binomial)
summary(fullBankFit)
predictions=predict(fullBankFit,newdata = Bank.fullTest,type="prob")
binaryPreds=ifelse(predictions[,2]>0.5,1,0)
binaryPredsfactor=factor(binaryPreds,levels=c(0,1),ordered=TRUE)
```

### Confusion matrix

```{r}
cm=confusionMatrix(Bank.fullTest$y,binaryPredsfactor)
cm
confusionTable=cm$table
```

### Calculate accuracy sensitivity, specificity.

```{r}
#accuracy
accuracy=cm$overall[1]
#true positive rate
sensitivity=cm$byClass[1]
#true negative rate
specificity=cm$byClass[2]
#mis classification
misclassificationRate=round(1-cm$overall[1],2)
cat("Misclassification Rate",misclassificationRate,"\n")
```

### ROC curve

```{r}
rocObj=roc(Bank.fullTest$y,binaryPredsfactor)
auc=auc(rocObj)
print(auc)
plot(rocObj,main="ROC curve",xlab="False Positive Rate",ylab="True Positive Rate",print.auc=TRUE,print.auc.x=0.5,print.auc.y=0.2)
abline(a=0,b=1,lty=2)
```

### Backward elimination

```{r}
fullBank=glm(y~.,data=Bank,family=binomial)
library(caret)
backwardsModel=step(
  object=fullBank,
  direction = "backward",
  scope=y~.,
  trace=0
)
selectedFeatures=names(coef(backwardsModel))[-1]
print(selectedFeatures)
```

### Forward selection

```{r}
forwardModel=step(
  #fullBank is a original model fit
  object=fullBank,
  direction = "forward",
  scope=y~.,
  trace=0
)
selectedFeatures=names(coef(backwardsModel))[-1]
print(selectedFeatures)
```

### Create a new data from the selected features(it comes from forward selection)

```{r}
selectedFeaturesCol=c("job","marital","education","housing","loan","contact","day","month","duration","campaign","pdays","previous","poutcome")
selected_features <- c(selectedFeaturesCol, "y")
print(selected_features)
newBank <- Bank%>%
  dplyr::select(one_of(selected_features))
```

### Creating training and testing data with selected features

```{r}
train = sample(dim(newBank)[1], dim(newBank)[1]*0.8)
test=-train
newBank.test=newBank[test,]
newBank.train=newBank[train,]
```

## (2)Fit the logistic regression model with selected features using k-fold cross-validation.

```{r}
library(caret)
fitControl2 <- trainControl(method = "cv", number = 10)
#fit the modle
newBankFit <- train(y ~ ., data = newBank.train , method = "glm", trControl = fitControl2,family=binomial)
summary(newBankFit)
#predicitons
predictions2=predict(newBankFit,newdata = newBank.test,type="prob")
binaryPreds2=ifelse(predictions2[,2]>0.5,1,0)
binaryPredsfactor2=factor(binaryPreds2,levels=c(0,1),ordered=TRUE)
```

### Confusion matrix for logit link

```{r}
cm2=confusionMatrix(newBank.test$y,binaryPredsfactor2)
confusionTable2=cm2$table
```

### Calculate accuracy sensitivity, specificity.

```{r}
#accuracy
accuracy2=cm2$overall[1]
cat("Accuracy:",accuracy2,"\n")
#true positive rate
sensitivity2=cm2$byClass[1]
cat("Sensitivity:",sensitivity2,"\n")
#true negative rate
specificity2=cm2$byClass[2]
cat("Specificity",specificity2,"\n")
#mis classification
misclassificationRate2=round(1-cm2$overall[1],2)
cat("Misclassification Rate",misclassificationRate2,"\n")
```

### ROC curve

```{r}
rocObj2=roc(newBank.test$y,binaryPredsfactor2)
auc2=auc(rocObj2)
print(auc2)
plot(rocObj2,main="ROC curve",xlab="False Positive Rate",ylab="True Positive Rate",print.auc=TRUE,print.auc.x=0.5,print.auc.y=0.2)
abline(a=0,b=1,lty=2)
```
## (3) Fit the model using random Forest machine learning model

```{r}
random.fit=train(y~.,data=newBank.train,method="rf",trControl=fitControl2)
prediction3=predict(random.fit,newdata=newBank.test,type="prob")
binaryPreds3=ifelse(prediction3[,2]>0.5,1,0)
binaryPredsfactor3=factor(binaryPreds3,levels=c(0,1),ordered=TRUE)
```
### Confusion Matrix

```{r}
cm3=confusionMatrix(binaryPredsfactor3,newBank.test$y)
cm3
confusionTable3=cm3$table
```
### Calculate accuracy sensitivity, specificity.

```{r}
#accuracy
accuracy3=cm3$overall[1]
cat("Accuracy:",accuracy3,"\n")
#true positive rate
sensitivity3=cm3$byClass[1]
cat("Sensitivity:",sensitivity3,"\n")
#true negative rate
specificity3=cm3$byClass[2]
cat("Specificity:",specificity3,"\n")
#mis classification
misclassificationRate3=1-accuracy3
cat("Misclassification Rate:",misclassificationRate3,"\n")
```
### Roc curve

```{r,warning=FALSE}
rocObj3=roc(newBank.test$y,binaryPredsfactor3)
auc3=auc(rocObj3)
print(auc3)
plot(rocObj3,main="ROC curve",xlab="False Positive Rate",ylab="True Positive Rate",print.auc=TRUE,print.auc.x=0.5,print.auc.y=0.2)
abline(a=0,b=1,lty=2)
```
## (4) Fit the model using Naive Bayes

```{r}
naiveBayesFit=train(y~.,data=newBank.train,method="naive_bayes",trControl=fitControl2)
predictions4=predict(naiveBayesFit,newdata=newBank.test)
```
### Confusion Matrix

```{r}
cm4=confusionMatrix(data=predictions4,newBank.test$y)
cm4
confusionTable4=cm4$table
```
### Calculate accuracy sensitivity, specificity.

```{r}
#accuracy
accuracy4=cm4$overall[1]
cat("Accuracy:",accuracy4,"\n")
#true positive rate
sensitivity4=cm4$byClass[1]
cat("Sensitivity:",sensitivity4,"\n")
#true negative rate
specificity4=cm4$byClass[2]
cat("Specificity:",specificity4,"\n")
#mis classification
misclassificationRate4=1-accuracy4
cat("Misclassification Rate:",misclassificationRate4,"\n")
```
### Roc curve

```{r,warning=FALSE}
response4=as.numeric(newBank.test$y)-1
predictor4=as.numeric(predictions4)-1
rocObj4=roc(response4,predictor4)
plot(rocObj4,print.auc=TRUE,print.auc.x=0.5,print.auc.y=0.2)
```

